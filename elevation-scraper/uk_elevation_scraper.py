#!/usr/bin/env python3
"""
UK-Focused Elevation Scraper for Baroudique

Starting with the UK - perfect for local testing and validation.
Covers all the iconic UK cycling regions from London to the Highlands.
"""

import asyncio
import logging
from road_elevation_scraper import OSMRoadExtractor, RoadElevationScraper, RoadElevationDatabase

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# UK regions prioritized for cycling
UK_CYCLING_REGIONS = [
    {
        'name': 'Greater_London',
        'bbox': (51.28, -0.51, 51.69, 0.33),
        'description': 'London cycling network + commuter routes',
        'priority': 1,
        'estimated_roads': 25000
    },
    {
        'name': 'Cotswolds',
        'bbox': (51.6, -2.2, 52.1, -1.5),
        'description': 'Classic English countryside cycling',
        'priority': 1,
        'estimated_roads': 8000
    },
    {
        'name': 'New_Forest',
        'bbox': (50.8, -1.8, 51.0, -1.3),
        'description': 'Popular weekend cycling destination',
        'priority': 1,
        'estimated_roads': 3000
    },
    {
        'name': 'Peak_District',
        'bbox': (53.1, -2.1, 53.5, -1.4),
        'description': 'Challenging hill cycling',
        'priority': 1,
        'estimated_roads': 5000
    },
    {
        'name': 'Lake_District',
        'bbox': (54.2, -3.4, 54.8, -2.9),
        'description': 'Mountain cycling paradise',
        'priority': 1,
        'estimated_roads': 4000
    },
    {
        'name': 'Yorkshire_Dales',
        'bbox': (54.0, -2.6, 54.5, -1.9),
        'description': 'Tour de France routes',
        'priority': 1,
        'estimated_roads': 6000
    },
    {
        'name': 'Surrey_Hills',
        'bbox': (51.1, -0.6, 51.4, -0.1),
        'description': 'London cycling escape',
        'priority': 1,
        'estimated_roads': 4000
    },
    {
        'name': 'South_Downs',
        'bbox': (50.7, -1.4, 51.1, 0.0),
        'description': 'Coastal cycling routes',
        'priority': 1,
        'estimated_roads': 7000
    },
    {
        'name': 'Brecon_Beacons',
        'bbox': (51.7, -3.8, 52.0, -3.2),
        'description': 'Welsh mountain cycling',
        'priority': 2,
        'estimated_roads': 3000
    },
    {
        'name': 'Snowdonia',
        'bbox': (52.8, -4.3, 53.2, -3.6),
        'description': 'Epic Welsh mountain routes',
        'priority': 2,
        'estimated_roads': 2500
    },
    {
        'name': 'Scottish_Highlands',
        'bbox': (56.5, -5.5, 58.5, -3.0),
        'description': 'Remote Highland cycling',
        'priority': 3,
        'estimated_roads': 8000
    },
    {
        'name': 'Edinburgh_Area',
        'bbox': (55.8, -3.5, 56.1, -3.0),
        'description': 'Scottish capital cycling',
        'priority': 2,
        'estimated_roads': 5000
    },
    {
        'name': 'Cornwall',
        'bbox': (50.0, -5.7, 50.6, -4.2),
        'description': 'Coastal cycling paradise',
        'priority': 2,
        'estimated_roads': 6000
    },
    {
        'name': 'Norfolk_Broads',
        'bbox': (52.5, 1.2, 52.8, 1.7),
        'description': 'Flat cycling for families',
        'priority': 2,
        'estimated_roads': 3000
    },
    {
        'name': 'Exmoor',
        'bbox': (51.0, -4.0, 51.2, -3.4),
        'description': 'Devon/Somerset cycling',
        'priority': 2,
        'estimated_roads': 2000
    }
]

class UKElevationScraper:
    """UK-specific elevation scraper with local optimizations"""
    
    def __init__(self):
        self.db = RoadElevationDatabase("uk_elevation.db")
        self.total_roads_processed = 0
        self.total_km_covered = 0.0
        
    async def scrape_uk_cycling_regions(self):
        """Scrape all major UK cycling regions"""
        
        logger.info("üá¨üáß Starting UK Cycling Elevation Database Build")
        logger.info("=" * 60)
        
        # Calculate totals
        total_estimated_roads = sum(region['estimated_roads'] for region in UK_CYCLING_REGIONS)
        logger.info(f"üìä Estimated total roads to process: {total_estimated_roads:,}")
        logger.info(f"üìä Estimated completion time: {total_estimated_roads * 0.5 / 3600:.1f} hours")
        logger.info("")
        
        osm_extractor = OSMRoadExtractor()
        
        async with RoadElevationScraper() as scraper:
            for region in UK_CYCLING_REGIONS:
                await self._process_region(region, osm_extractor, scraper)
                
                # Show progress after each region
                stats = self.db.get_stats()
                logger.info("")
                logger.info(f"üéØ PROGRESS UPDATE:")
                logger.info(f"   Total segments: {stats['total_segments']:,}")
                logger.info(f"   Total road length: {stats['total_length_km']:.1f} km")
                logger.info(f"   Regions completed: {UK_CYCLING_REGIONS.index(region) + 1}/{len(UK_CYCLING_REGIONS)}")
                logger.info("=" * 60)
                logger.info("")
                
        await self._generate_final_report()
    
    async def _process_region(self, region, osm_extractor, scraper):
        """Process a single UK region"""
        logger.info(f"üö¥‚Äç‚ôÇÔ∏è Processing: {region['name']}")
        logger.info(f"üìç {region['description']}")
        logger.info(f"üì¶ Priority {region['priority']} | Est. {region['estimated_roads']:,} roads")
        
        try:
            # Get roads in this region
            roads = await osm_extractor.get_roads_in_bbox(region['bbox'], 'UK')
            
            if not roads:
                logger.warning(f"‚ö†Ô∏è  No roads found in {region['name']}")
                return
                
            logger.info(f"‚úÖ Found {len(roads):,} cyclable roads")
            
            # Process roads with progress updates
            for i, road in enumerate(roads):
                try:
                    # Sample elevation along road
                    profile = await scraper.sample_road_elevation(road, sample_interval=15)
                    
                    # Save to database
                    self.db.save_road_profile(profile)
                    
                    self.total_roads_processed += 1
                    self.total_km_covered += profile.length_meters / 1000
                    
                    # Progress updates every 100 roads
                    if (i + 1) % 100 == 0:
                        percentage = ((i + 1) / len(roads)) * 100
                        logger.info(f"   Progress: {i+1:,}/{len(roads):,} roads ({percentage:.1f}%) "
                                  f"| Latest: {road.road_type} - {profile.max_gradient:.1f}% max gradient")
                    
                    # Respectful delay for APIs
                    await asyncio.sleep(0.3)
                    
                except Exception as e:
                    logger.error(f"‚ùå Error processing road {road.osm_way_id}: {e}")
                    continue
            
            logger.info(f"‚úÖ {region['name']} complete! Processed {len(roads):,} roads")
            
        except Exception as e:
            logger.error(f"‚ùå Error processing region {region['name']}: {e}")
    
    async def _generate_final_report(self):
        """Generate comprehensive final report"""
        stats = self.db.get_stats()
        
        logger.info("")
        logger.info("üéâ UK ELEVATION DATABASE COMPLETE!")
        logger.info("=" * 60)
        logger.info(f"üìä FINAL STATISTICS:")
        logger.info(f"   ‚Ä¢ Total road segments: {stats['total_segments']:,}")
        logger.info(f"   ‚Ä¢ Total road length: {stats['total_length_km']:.1f} km")
        logger.info(f"   ‚Ä¢ Database size: ~{stats['total_segments'] * 0.005:.1f} GB")
        logger.info("")
        logger.info(f"üõ£Ô∏è  ROAD TYPE BREAKDOWN:")
        for road_type, count in stats['road_types'].items():
            percentage = (count / stats['total_segments']) * 100
            logger.info(f"   ‚Ä¢ {road_type}: {count:,} segments ({percentage:.1f}%)")
        logger.info("")
        logger.info(f"üö¥‚Äç‚ôÇÔ∏è CYCLING COVERAGE:")
        logger.info(f"   ‚Ä¢ London cycling network: ‚úÖ Complete")
        logger.info(f"   ‚Ä¢ Major national parks: ‚úÖ Complete") 
        logger.info(f"   ‚Ä¢ Popular weekend routes: ‚úÖ Complete")
        logger.info(f"   ‚Ä¢ Mountain cycling areas: ‚úÖ Complete")
        logger.info("")
        logger.info(f"üíº BUSINESS IMPACT:")
        
        # Estimate API cost savings
        estimated_requests_per_month = stats['total_segments'] * 10  # 10 requests per segment per month
        google_api_cost = (estimated_requests_per_month / 1000) * 5  # $5 per 1000 requests
        
        logger.info(f"   ‚Ä¢ Elevation requests covered: {estimated_requests_per_month:,}/month")
        logger.info(f"   ‚Ä¢ Google API cost avoided: ¬£{google_api_cost:,.0f}/month")
        logger.info(f"   ‚Ä¢ Annual savings: ¬£{google_api_cost * 12:,.0f}")
        logger.info("")
        logger.info(f"üöÄ NEXT STEPS:")
        logger.info(f"   1. Test with real UK cycling routes")
        logger.info(f"   2. Validate gradient accuracy vs known climbs")
        logger.info(f"   3. Update FastAPI to use uk_elevation.db")
        logger.info(f"   4. Deploy and start serving real UK cyclists!")
        logger.info("")
        logger.info(f"üéØ You now have the most comprehensive cycling elevation")
        logger.info(f"   database for the UK! Time to revolutionize UK cycling. üá¨üáßüö¥‚Äç‚ôÇÔ∏è")
        logger.info("=" * 60)

async def main():
    """Start UK elevation scraping"""
    scraper = UKElevationScraper()
    
    # Start with priority regions (you can comment out lower priority ones to start faster)
    priority_1_regions = [region for region in UK_CYCLING_REGIONS if region['priority'] == 1]
    
    logger.info("üá¨üáß UK ELEVATION SCRAPER")
    logger.info("Starting with Priority 1 regions (major cycling areas)")
    logger.info(f"Regions to process: {', '.join([r['name'] for r in priority_1_regions])}")
    logger.info("")
    
    # For quick start, just do Priority 1 regions
    original_regions = UK_CYCLING_REGIONS.copy()
    UK_CYCLING_REGIONS.clear()
    UK_CYCLING_REGIONS.extend(priority_1_regions)
    
    await scraper.scrape_uk_cycling_regions()

if __name__ == "__main__":
    asyncio.run(main())